{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ismat-Samadov/Named_Entity_Recognition/blob/main/notebooks/train_trocr_iam_correct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {
        "id": "cell-0"
      },
      "source": [
        "### Dataset Structure Understanding:\n",
        "Each IAM image contains **TWO parts**:\n",
        "1. **TOP half**: Machine-printed text (ground truth)\n",
        "2. **BOTTOM half**: Handwritten copy of the same text\n",
        "\n",
        "### Our Approach:\n",
        "1. Split each image horizontally (top/bottom)\n",
        "2. OCR the printed text (top) to extract ground truth labels\n",
        "3. Use handwritten part (bottom) as training input\n",
        "4. Train TrOCR: Handwritten Image → Printed Text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {
        "id": "cell-1"
      },
      "source": [
        "## 1. Setup and Installation\n",
        "\n",
        "Install required packages for Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "id": "lt6tim2kxy",
      "source": [
        "# Install required packages\n",
        "!pip install -q kagglehub transformers datasets pillow opencv-python-headless\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install -q pytesseract jiwer easyocr\n",
        "!apt-get install -y tesseract-ocr\n",
        "\n",
        "print(\"✓ All packages installed successfully!\")"
      ],
      "metadata": {
        "id": "lt6tim2kxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "thaqmadowwg",
      "source": [
        "# Imports\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    VisionEncoderDecoderModel,\n",
        "    TrOCRProcessor,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "from jiwer import cer, wer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try importing EasyOCR for better printed text extraction\n",
        "try:\n",
        "    import easyocr\n",
        "    USE_EASYOCR = True\n",
        "except ImportError:\n",
        "    USE_EASYOCR = False\n",
        "    print(\"EasyOCR not available, will use pytesseract\")\n",
        "\n",
        "# Set random seeds\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (15, 8)\n",
        "\n",
        "print(\"✓ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'}\")"
      ],
      "metadata": {
        "id": "thaqmadowwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "h1mtpvta339",
      "source": [
        "# Analyze dataset statistics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "\n",
        "# Get all image files\n",
        "all_images = list(data_dir.glob(\"**/*.png\"))\n",
        "if len(all_images) == 0:\n",
        "    all_images = list(data_dir.glob(\"*.png\"))\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"IAM DATASET STATISTICS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Total images: {len(all_images)}\")\n",
        "\n",
        "# Analyze image dimensions\n",
        "widths = []\n",
        "heights = []\n",
        "aspect_ratios = []\n",
        "\n",
        "sample_size = min(100, len(all_images))\n",
        "for img_path in all_images[:sample_size]:\n",
        "    img = Image.open(img_path)\n",
        "    w, h = img.size\n",
        "    widths.append(w)\n",
        "    heights.append(h)\n",
        "    aspect_ratios.append(w/h)\n",
        "\n",
        "print(f\"\\nImage Dimensions (sampled {sample_size} images):\")\n",
        "print(f\"  Width:  {np.mean(widths):.0f} ± {np.std(widths):.0f} px (range: {min(widths)}-{max(widths)})\")\n",
        "print(f\"  Height: {np.mean(heights):.0f} ± {np.std(heights):.0f} px (range: {min(heights)}-{max(heights)})\")\n",
        "print(f\"  Aspect Ratio: {np.mean(aspect_ratios):.2f} ± {np.std(aspect_ratios):.2f}\")\n",
        "\n",
        "# Analyze directory structure\n",
        "writers = {}\n",
        "for img_path in all_images:\n",
        "    if img_path.parent != data_dir:\n",
        "        writer_id = img_path.parent.name\n",
        "        if writer_id not in writers:\n",
        "            writers[writer_id] = 0\n",
        "        writers[writer_id] += 1\n",
        "\n",
        "if writers:\n",
        "    print(f\"\\nWriter Distribution:\")\n",
        "    print(f\"  Total writers: {len(writers)}\")\n",
        "    print(f\"  Images per writer: {np.mean(list(writers.values())):.1f} ± {np.std(list(writers.values())):.1f}\")\n",
        "    print(f\"  Min images per writer: {min(writers.values())}\")\n",
        "    print(f\"  Max images per writer: {max(writers.values())}\")\n",
        "\n",
        "# Visualize distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Width distribution\n",
        "axes[0].hist(widths, bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "axes[0].set_xlabel('Width (pixels)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Image Width Distribution')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Height distribution\n",
        "axes[1].hist(heights, bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
        "axes[1].set_xlabel('Height (pixels)')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "axes[1].set_title('Image Height Distribution')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Aspect ratio distribution\n",
        "axes[2].hist(aspect_ratios, bins=30, color='lightgreen', edgecolor='black', alpha=0.7)\n",
        "axes[2].set_xlabel('Aspect Ratio (W/H)')\n",
        "axes[2].set_ylabel('Frequency')\n",
        "axes[2].set_title('Aspect Ratio Distribution')\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATASET CHARACTERISTICS:\")\n",
        "print(\"=\" * 80)\n",
        "print(\"✓ IAM Handwritten Forms Dataset\")\n",
        "print(\"✓ Contains handwritten copies of printed text\")\n",
        "print(\"✓ Each image has printed (top) and handwritten (bottom) versions\")\n",
        "print(\"✓ Multiple writers with different handwriting styles\")\n",
        "print(\"✓ Useful for training handwriting recognition models\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "h1mtpvta339"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "id": "za8vstuhn3",
      "source": [
        "# Display sample images from the dataset\n",
        "import random\n",
        "\n",
        "# Get random sample of images\n",
        "sample_images = random.sample(all_images, min(6, len(all_images)))\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, img_path in enumerate(sample_images):\n",
        "    img = Image.open(img_path)\n",
        "    axes[idx].imshow(img, cmap='gray')\n",
        "\n",
        "    # Get writer ID if available\n",
        "    if img_path.parent != data_dir:\n",
        "        writer_info = f\"Writer: {img_path.parent.name}\"\n",
        "    else:\n",
        "        writer_info = \"Dataset Sample\"\n",
        "\n",
        "    axes[idx].set_title(f\"{writer_info}\\nSize: {img.size[0]}x{img.size[1]}\", fontsize=10)\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "    # Add annotation for the split\n",
        "    height = img.size[1]\n",
        "    axes[idx].axhline(y=height//2, color='red', linestyle='--', linewidth=1.5, alpha=0.7)\n",
        "\n",
        "plt.suptitle('Sample Images from IAM Dataset (Red line shows printed/handwritten split)',\n",
        "             fontsize=14, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Sample images displayed successfully!\")\n",
        "print(\"Note: The red dashed line shows where we'll split the image:\")\n",
        "print(\"  - TOP half: Printed text (used as ground truth labels)\")\n",
        "print(\"  - BOTTOM half: Handwritten text (used as training input)\")"
      ],
      "metadata": {
        "id": "za8vstuhn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "8uyu9iwieoe",
      "source": [
        "## Sample Images from Dataset\n",
        "\n",
        "Let's visualize some sample images to understand the dataset structure"
      ],
      "metadata": {
        "id": "8uyu9iwieoe"
      }
    },
    {
      "cell_type": "markdown",
      "id": "wgcxacgbjq",
      "source": [
        "## Dataset Analysis and Statistics\n",
        "\n",
        "Let's analyze the IAM dataset to understand its characteristics"
      ],
      "metadata": {
        "id": "wgcxacgbjq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-3",
      "metadata": {
        "id": "cell-3",
        "outputId": "f5e90e64-a744-4802-eb18-030739183713"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "dlopen(/Users/ismatsamadov/handwriting_data_processing/venv/lib/python3.10/site-packages/torch/_C.cpython-310-darwin.so, 0x0002): Library not loaded: @rpath/libtorch_cpu.dylib\n  Referenced from: <8C033F9E-F29C-3CF8-80FD-7D03760F3A30> /Users/ismatsamadov/handwriting_data_processing/venv/lib/python3.10/site-packages/torch/lib/libtorch_python.dylib\n  Reason: tried: '/Users/ismatsamadov/handwriting_data_processing/venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/Users/runner/work/_temp/anaconda/envs/wheel_py310/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/_temp/anaconda/envs/wheel_py310/lib/libtorch_cpu.dylib' (no such file), '/Users/ismatsamadov/handwriting_data_processing/venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/Users/runner/work/_temp/anaconda/envs/wheel_py310/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/_temp/anaconda/envs/wheel_py310/lib/libtorch_cpu.dylib' (no such file), '/Users/ismatsamadov/handwriting_data_processing/venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/Users/ismatsamadov/.pyenv/versions/3.10.12/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/ismatsamadov/.pyenv/versions/3.10.12/lib/libtorch_cpu.dylib' (no such file), '/opt/homebrew/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libtorch_cpu.dylib' (no such file), '/Users/ismatsamadov/.pyenv/versions/3.10.12/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/ismatsamadov/.pyenv/versions/3.10.12/lib/libtorch_cpu.dylib' (no such file), '/opt/homebrew/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libtorch_cpu.dylib' (no such file)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     VisionEncoderDecoderModel,\n\u001b[1;32m     15\u001b[0m     TrOCRProcessor,\n\u001b[1;32m     16\u001b[0m     get_linear_schedule_with_warmup\n\u001b[1;32m     17\u001b[0m )\n",
            "File \u001b[0;32m~/handwriting_data_processing/venv/lib/python3.10/site-packages/torch/__init__.py:427\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    426\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 427\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mSymInt\u001b[39;00m:\n\u001b[1;32m    431\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;124;03m    Like an int (including magic methods), but redirects all operations on the\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;124;03m    wrapped node. This is used in particular to symbolically record operations\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;124;03m    in the symbolic shape workflow.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
            "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/ismatsamadov/handwriting_data_processing/venv/lib/python3.10/site-packages/torch/_C.cpython-310-darwin.so, 0x0002): Library not loaded: @rpath/libtorch_cpu.dylib\n  Referenced from: <8C033F9E-F29C-3CF8-80FD-7D03760F3A30> /Users/ismatsamadov/handwriting_data_processing/venv/lib/python3.10/site-packages/torch/lib/libtorch_python.dylib\n  Reason: tried: '/Users/ismatsamadov/handwriting_data_processing/venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/Users/runner/work/_temp/anaconda/envs/wheel_py310/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/_temp/anaconda/envs/wheel_py310/lib/libtorch_cpu.dylib' (no such file), '/Users/ismatsamadov/handwriting_data_processing/venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/Users/runner/work/_temp/anaconda/envs/wheel_py310/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/runner/work/_temp/anaconda/envs/wheel_py310/lib/libtorch_cpu.dylib' (no such file), '/Users/ismatsamadov/handwriting_data_processing/venv/lib/python3.10/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/Users/ismatsamadov/.pyenv/versions/3.10.12/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/ismatsamadov/.pyenv/versions/3.10.12/lib/libtorch_cpu.dylib' (no such file), '/opt/homebrew/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libtorch_cpu.dylib' (no such file), '/Users/ismatsamadov/.pyenv/versions/3.10.12/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/Users/ismatsamadov/.pyenv/versions/3.10.12/lib/libtorch_cpu.dylib' (no such file), '/opt/homebrew/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libtorch_cpu.dylib' (no such file)"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    VisionEncoderDecoderModel,\n",
        "    TrOCRProcessor,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "from jiwer import cer, wer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try importing EasyOCR for better printed text extraction\n",
        "try:\n",
        "    import easyocr\n",
        "    USE_EASYOCR = True\n",
        "except ImportError:\n",
        "    USE_EASYOCR = False\n",
        "    print(\"EasyOCR not available, will use pytesseract\")\n",
        "\n",
        "# Set random seeds\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set style\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (15, 8)\n",
        "\n",
        "print(\"✓ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Device: {'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {
        "id": "cell-4"
      },
      "source": [
        "# Load and display sample images to understand structure\n",
        "# Use data_dir from the download step\n",
        "sample_images = list(data_dir.glob(\"*/*.png\"))[:6]\n",
        "\n",
        "if len(sample_images) == 0:\n",
        "    # Try without subdirectories\n",
        "    sample_images = list(data_dir.glob(\"*.png\"))[:6]\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, img_path in enumerate(sample_images):\n",
        "    img = Image.open(img_path)\n",
        "    axes[idx].imshow(img, cmap='gray')\n",
        "    axes[idx].set_title(f\"Writer {img_path.parent.name} - {img_path.name}\")\n",
        "    axes[idx].axis('off')\n",
        "    \n",
        "    # Draw a horizontal line to show the split point\n",
        "    height = img.size[1]\n",
        "    axes[idx].axhline(y=height//2, color='red', linestyle='--', linewidth=2, label='Split line')\n",
        "    axes[idx].text(10, height//4, 'PRINTED', color='blue', fontsize=12, weight='bold',\n",
        "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "    axes[idx].text(10, 3*height//4, 'HANDWRITTEN', color='green', fontsize=12, weight='bold',\n",
        "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.suptitle('IAM Dataset Structure: TOP = Printed, BOTTOM = Handwritten', fontsize=16, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET STRUCTURE CONFIRMED:\")\n",
        "print(\"=\"*80)\n",
        "print(\"✓ Each image has TWO parts:\")\n",
        "print(\"  1. TOP half    : Machine-printed text (GROUND TRUTH)\")\n",
        "print(\"  2. BOTTOM half : Handwritten text (INPUT for training)\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-5",
      "metadata": {
        "id": "cell-5"
      },
      "outputs": [],
      "source": [
        "# Load and display sample images to understand structure\n",
        "data_dir = Path(\"../archive/data\")\n",
        "sample_images = list(data_dir.glob(\"*/*.png\"))[:6]\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, img_path in enumerate(sample_images):\n",
        "    img = Image.open(img_path)\n",
        "    axes[idx].imshow(img, cmap='gray')\n",
        "    axes[idx].set_title(f\"Writer {img_path.parent.name} - {img_path.name}\")\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "    # Draw a horizontal line to show the split point\n",
        "    height = img.size[1]\n",
        "    axes[idx].axhline(y=height//2, color='red', linestyle='--', linewidth=2, label='Split line')\n",
        "    axes[idx].text(10, height//4, 'PRINTED', color='blue', fontsize=12, weight='bold',\n",
        "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "    axes[idx].text(10, 3*height//4, 'HANDWRITTEN', color='green', fontsize=12, weight='bold',\n",
        "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "plt.suptitle('IAM Dataset Structure: TOP = Printed, BOTTOM = Handwritten', fontsize=16, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATASET STRUCTURE CONFIRMED:\")\n",
        "print(\"=\"*80)\n",
        "print(\"✓ Each image has TWO parts:\")\n",
        "print(\"  1. TOP half    : Machine-printed text (GROUND TRUTH)\")\n",
        "print(\"  2. BOTTOM half : Handwritten text (INPUT for training)\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-6",
      "metadata": {
        "id": "cell-6"
      },
      "source": [
        "## 3. Image Splitting Functions\n",
        "\n",
        "Create functions to split images and extract text from printed part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-7",
      "metadata": {
        "id": "cell-7"
      },
      "outputs": [],
      "source": [
        "def split_image(image_path):\n",
        "    \"\"\"\n",
        "    Split IAM image into printed (top) and handwritten (bottom) parts.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to IAM image\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (printed_image, handwritten_image)\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    width, height = img.size\n",
        "\n",
        "    # Split at middle\n",
        "    split_point = height // 2\n",
        "\n",
        "    # Top half = printed text\n",
        "    printed_img = img.crop((0, 0, width, split_point))\n",
        "\n",
        "    # Bottom half = handwritten text\n",
        "    handwritten_img = img.crop((0, split_point, width, height))\n",
        "\n",
        "    return printed_img, handwritten_img\n",
        "\n",
        "\n",
        "def extract_text_from_printed(printed_img, method='easyocr'):\n",
        "    \"\"\"\n",
        "    Extract text from printed portion using OCR.\n",
        "\n",
        "    Args:\n",
        "        printed_img: PIL Image of printed text\n",
        "        method: 'easyocr' or 'tesseract'\n",
        "\n",
        "    Returns:\n",
        "        Extracted text string\n",
        "    \"\"\"\n",
        "    if method == 'easyocr' and USE_EASYOCR:\n",
        "        # Use EasyOCR (more accurate for printed text)\n",
        "        reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
        "        img_np = np.array(printed_img)\n",
        "        results = reader.readtext(img_np, detail=0)\n",
        "        text = ' '.join(results)\n",
        "    else:\n",
        "        # Fallback to Tesseract\n",
        "        import pytesseract\n",
        "        text = pytesseract.image_to_string(printed_img)\n",
        "        text = text.strip()\n",
        "\n",
        "    # Clean up text\n",
        "    text = ' '.join(text.split())  # Remove extra whitespace\n",
        "    return text\n",
        "\n",
        "\n",
        "# Test the functions\n",
        "print(\"Testing image splitting...\\n\")\n",
        "test_img_path = sample_images[0]\n",
        "printed, handwritten = split_image(test_img_path)\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Original\n",
        "axes[0].imshow(Image.open(test_img_path), cmap='gray')\n",
        "axes[0].set_title('Original Image', fontsize=14, weight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Printed\n",
        "axes[1].imshow(printed, cmap='gray')\n",
        "axes[1].set_title('Printed Text (TOP) → Ground Truth', fontsize=14, weight='bold', color='blue')\n",
        "axes[1].axis('off')\n",
        "\n",
        "# Handwritten\n",
        "axes[2].imshow(handwritten, cmap='gray')\n",
        "axes[2].set_title('Handwritten Text (BOTTOM) → Training Input', fontsize=14, weight='bold', color='green')\n",
        "axes[2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Image splitting successful!\")\n",
        "print(f\"Printed image size: {printed.size}\")\n",
        "print(f\"Handwritten image size: {handwritten.size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-8",
      "metadata": {
        "id": "cell-8"
      },
      "source": [
        "## 4. Extract Ground Truth Labels\n",
        "\n",
        "Use OCR to extract text from printed portions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-9",
      "metadata": {
        "id": "cell-9"
      },
      "outputs": [],
      "source": [
        "# Initialize EasyOCR reader (do this once)\n",
        "if USE_EASYOCR:\n",
        "    print(\"Initializing EasyOCR...\")\n",
        "    ocr_reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
        "    print(\"✓ EasyOCR ready!\")\n",
        "else:\n",
        "    print(\"Using Tesseract OCR\")\n",
        "    ocr_reader = None\n",
        "\n",
        "# Test OCR extraction\n",
        "print(\"\\nTesting text extraction from printed portion...\\n\")\n",
        "test_printed, test_handwritten = split_image(sample_images[0])\n",
        "\n",
        "# Extract text\n",
        "if USE_EASYOCR:\n",
        "    img_np = np.array(test_printed)\n",
        "    results = ocr_reader.readtext(img_np, detail=0)\n",
        "    ground_truth = ' '.join(results)\n",
        "else:\n",
        "    import pytesseract\n",
        "    ground_truth = pytesseract.image_to_string(test_printed).strip()\n",
        "\n",
        "ground_truth = ' '.join(ground_truth.split())  # Clean\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"EXTRACTED GROUND TRUTH:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{ground_truth}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nLength: {len(ground_truth)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-10",
      "metadata": {
        "id": "cell-10"
      },
      "source": [
        "## 5. Create Correct IAM Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-11",
      "metadata": {
        "id": "cell-11"
      },
      "outputs": [],
      "source": [
        "class IAMHandwritingDataset(Dataset):\n",
        "    \"\"\"\n",
        "    IAM Dataset with correct understanding:\n",
        "    - Splits each image into printed (top) and handwritten (bottom)\n",
        "    - Extracts ground truth from printed text\n",
        "    - Uses handwritten part for training\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, split='train', processor=None, num_samples=None, cache_labels=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir: Path to archive/data\n",
        "            split: 'train', 'val', or 'test'\n",
        "            processor: TrOCR processor\n",
        "            num_samples: Limit samples (for testing)\n",
        "            cache_labels: Cache extracted labels to speed up training\n",
        "        \"\"\"\n",
        "        self.data_dir = Path(data_dir)\n",
        "        self.split = split\n",
        "        self.processor = processor\n",
        "        self.cache_labels = cache_labels\n",
        "        self.label_cache = {}\n",
        "\n",
        "        # Initialize OCR reader\n",
        "        if USE_EASYOCR:\n",
        "            self.ocr_reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n",
        "        else:\n",
        "            self.ocr_reader = None\n",
        "\n",
        "        # Load image paths\n",
        "        all_images = list(self.data_dir.glob(\"*/*.png\"))\n",
        "        random.seed(42)\n",
        "        random.shuffle(all_images)\n",
        "\n",
        "        # Split into train/val/test\n",
        "        total = len(all_images)\n",
        "        train_end = int(total * 0.8)\n",
        "        val_end = int(total * 0.9)\n",
        "\n",
        "        if split == 'train':\n",
        "            selected = all_images[:train_end]\n",
        "        elif split == 'val':\n",
        "            selected = all_images[train_end:val_end]\n",
        "        else:  # test\n",
        "            selected = all_images[val_end:]\n",
        "\n",
        "        if num_samples:\n",
        "            selected = selected[:num_samples]\n",
        "\n",
        "        self.image_paths = selected\n",
        "        print(f\"✓ Loaded {len(self.image_paths)} images for {split} split\")\n",
        "\n",
        "    def _extract_label(self, image_path):\n",
        "        \"\"\"Extract ground truth label from printed portion.\"\"\"\n",
        "        # Check cache\n",
        "        if self.cache_labels and str(image_path) in self.label_cache:\n",
        "            return self.label_cache[str(image_path)]\n",
        "\n",
        "        # Split image\n",
        "        printed_img, _ = split_image(image_path)\n",
        "\n",
        "        # Extract text using OCR\n",
        "        if USE_EASYOCR and self.ocr_reader:\n",
        "            img_np = np.array(printed_img)\n",
        "            results = self.ocr_reader.readtext(img_np, detail=0)\n",
        "            text = ' '.join(results)\n",
        "        else:\n",
        "            import pytesseract\n",
        "            text = pytesseract.image_to_string(printed_img).strip()\n",
        "\n",
        "        # Clean text\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        # Cache\n",
        "        if self.cache_labels:\n",
        "            self.label_cache[str(image_path)] = text\n",
        "\n",
        "        return text\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "\n",
        "        # Split image and get handwritten part\n",
        "        _, handwritten_img = split_image(image_path)\n",
        "\n",
        "        # Extract ground truth from printed part\n",
        "        text = self._extract_label(image_path)\n",
        "\n",
        "        # If text is empty or too short, skip\n",
        "        if len(text.strip()) < 3:\n",
        "            text = \"sample text\"  # Fallback\n",
        "\n",
        "        if self.processor:\n",
        "            # Process handwritten image\n",
        "            pixel_values = self.processor(handwritten_img, return_tensors=\"pt\").pixel_values.squeeze()\n",
        "\n",
        "            # Process text label\n",
        "            labels = self.processor.tokenizer(\n",
        "                text,\n",
        "                padding=\"max_length\",\n",
        "                max_length=128,\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            ).input_ids.squeeze()\n",
        "\n",
        "            # Replace padding with -100 for loss calculation\n",
        "            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "            return {\n",
        "                'pixel_values': pixel_values,\n",
        "                'labels': labels,\n",
        "                'text': text\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'image': handwritten_img,\n",
        "                'text': text\n",
        "            }\n",
        "\n",
        "\n",
        "print(\"✓ Dataset class created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-12",
      "metadata": {
        "id": "cell-12"
      },
      "source": [
        "## 6. Initialize TrOCR Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-13",
      "metadata": {
        "id": "cell-13"
      },
      "outputs": [],
      "source": [
        "# Load TrOCR model and processor\n",
        "print(\"Loading TrOCR model...\")\n",
        "model_name = \"microsoft/trocr-base-handwritten\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "\n",
        "processor = TrOCRProcessor.from_pretrained(model_name)\n",
        "model = VisionEncoderDecoderModel.from_pretrained(model_name)\n",
        "\n",
        "# IMPORTANT: Set decoder_start_token_id and pad_token_id\n",
        "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "print(f\"✓ Model loaded on {device}\")\n",
        "print(f\"\\nModel Architecture:\")\n",
        "print(f\"  Encoder: {model.config.encoder.model_type} ({model.encoder.num_parameters():,} params)\")\n",
        "print(f\"  Decoder: {model.config.decoder.model_type} ({model.decoder.num_parameters():,} params)\")\n",
        "print(f\"  Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  Decoder Start Token ID: {model.config.decoder_start_token_id}\")\n",
        "print(f\"  Pad Token ID: {model.config.pad_token_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-14",
      "metadata": {
        "id": "cell-14"
      },
      "source": [
        "# Create datasets (start with small sample for testing)\n",
        "print(\"Creating datasets...\\n\")\n",
        "\n",
        "train_dataset = IAMHandwritingDataset(\n",
        "    data_dir,  # Use the downloaded dataset path\n",
        "    split='train',\n",
        "    processor=processor,\n",
        "    num_samples=50  # Start small for testing\n",
        ")\n",
        "\n",
        "val_dataset = IAMHandwritingDataset(\n",
        "    data_dir,  # Use the downloaded dataset path\n",
        "    split='val',\n",
        "    processor=processor,\n",
        "    num_samples=10\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"\\n✓ DataLoaders created\")\n",
        "print(f\"  Train samples: {len(train_dataset)}\")\n",
        "print(f\"  Val samples: {len(val_dataset)}\")\n",
        "print(f\"  Batch size: 4\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n",
        "\n",
        "# Show sample batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"\\nSample Batch:\")\n",
        "print(f\"  pixel_values shape: {sample_batch['pixel_values'].shape}\")\n",
        "print(f\"  labels shape: {sample_batch['labels'].shape}\")\n",
        "print(f\"\\nSample Ground Truth Texts:\")\n",
        "for i, text in enumerate(sample_batch['text'][:3]):\n",
        "    print(f\"  {i+1}. {text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-15",
      "metadata": {
        "id": "cell-15"
      },
      "outputs": [],
      "source": [
        "# Create datasets (start with small sample for testing)\n",
        "print(\"Creating datasets...\\n\")\n",
        "\n",
        "train_dataset = IAMHandwritingDataset(\n",
        "    \"../archive/data\",\n",
        "    split='train',\n",
        "    processor=processor,\n",
        "    num_samples=50  # Start small for testing\n",
        ")\n",
        "\n",
        "val_dataset = IAMHandwritingDataset(\n",
        "    \"../archive/data\",\n",
        "    split='val',\n",
        "    processor=processor,\n",
        "    num_samples=10\n",
        ")\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"\\n✓ DataLoaders created\")\n",
        "print(f\"  Train samples: {len(train_dataset)}\")\n",
        "print(f\"  Val samples: {len(val_dataset)}\")\n",
        "print(f\"  Batch size: 4\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n",
        "\n",
        "# Show sample batch\n",
        "sample_batch = next(iter(train_loader))\n",
        "print(f\"\\nSample Batch:\")\n",
        "print(f\"  pixel_values shape: {sample_batch['pixel_values'].shape}\")\n",
        "print(f\"  labels shape: {sample_batch['labels'].shape}\")\n",
        "print(f\"\\nSample Ground Truth Texts:\")\n",
        "for i, text in enumerate(sample_batch['text'][:3]):\n",
        "    print(f\"  {i+1}. {text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-16",
      "metadata": {
        "id": "cell-16"
      },
      "source": [
        "# Visualize training samples\n",
        "fig, axes = plt.subplots(3, 2, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Get samples without processing\n",
        "vis_dataset = IAMHandwritingDataset(\n",
        "    data_dir,  # Use the downloaded dataset path\n",
        "    split='train',\n",
        "    processor=None,\n",
        "    num_samples=6\n",
        ")\n",
        "\n",
        "for idx in range(6):\n",
        "    sample = vis_dataset[idx]\n",
        "    img = sample['image']\n",
        "    text = sample['text']\n",
        "    \n",
        "    axes[idx].imshow(img, cmap='gray')\n",
        "    axes[idx].set_title(f\"Ground Truth: {text[:50]}...\", fontsize=10, wrap=True)\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Handwritten Images (Training Input) with Ground Truth Labels (from printed text)',\n",
        "             fontsize=14, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING SETUP:\")\n",
        "print(\"=\"*80)\n",
        "print(\"INPUT:  Handwritten images (bottom half)\")\n",
        "print(\"OUTPUT: Text labels (extracted from printed top half)\")\n",
        "print(\"GOAL:   Train TrOCR to recognize handwriting\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-17",
      "metadata": {
        "id": "cell-17"
      },
      "outputs": [],
      "source": [
        "# Visualize training samples\n",
        "fig, axes = plt.subplots(3, 2, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Get samples without processing\n",
        "vis_dataset = IAMHandwritingDataset(\n",
        "    \"../archive/data\",\n",
        "    split='train',\n",
        "    processor=None,\n",
        "    num_samples=6\n",
        ")\n",
        "\n",
        "for idx in range(6):\n",
        "    sample = vis_dataset[idx]\n",
        "    img = sample['image']\n",
        "    text = sample['text']\n",
        "\n",
        "    axes[idx].imshow(img, cmap='gray')\n",
        "    axes[idx].set_title(f\"Ground Truth: {text[:50]}...\", fontsize=10, wrap=True)\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Handwritten Images (Training Input) with Ground Truth Labels (from printed text)',\n",
        "             fontsize=14, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING SETUP:\")\n",
        "print(\"=\"*80)\n",
        "print(\"INPUT:  Handwritten images (bottom half)\")\n",
        "print(\"OUTPUT: Text labels (extracted from printed top half)\")\n",
        "print(\"GOAL:   Train TrOCR to recognize handwriting\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-18",
      "metadata": {
        "id": "cell-18"
      },
      "source": [
        "## 9. Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-19",
      "metadata": {
        "id": "cell-19"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "NUM_EPOCHS = 5\n",
        "LEARNING_RATE = 5e-5\n",
        "WARMUP_STEPS = 20\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "total_steps = len(train_loader) * NUM_EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "print(\"Training Configuration:\")\n",
        "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
        "print(f\"  Warmup Steps: {WARMUP_STEPS}\")\n",
        "print(f\"  Total Steps: {total_steps}\")\n",
        "print(f\"  Optimizer: AdamW\")\n",
        "print(f\"  Device: {device}\")\n",
        "print(f\"\\n  Dataset Understanding:\")\n",
        "print(f\"    ✓ Handwritten images as input\")\n",
        "print(f\"    ✓ Printed text (OCR extracted) as labels\")\n",
        "print(f\"    ✓ Each sample: Handwritten → Printed Text\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-20",
      "metadata": {
        "id": "cell-20"
      },
      "source": [
        "## 10. Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-21",
      "metadata": {
        "id": "cell-21"
      },
      "outputs": [],
      "source": [
        "# Training history\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'val_loss': [],\n",
        "    'val_cer': [],\n",
        "    'val_wer': [],\n",
        "    'learning_rate': []\n",
        "}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_train_loss = train_loss / len(train_loader)\n",
        "    history['train_loss'].append(avg_train_loss)\n",
        "    history['learning_rate'].append(scheduler.get_last_lr()[0])\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_preds = []\n",
        "    all_refs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "            val_loss += outputs.loss.item()\n",
        "\n",
        "            # Generate predictions\n",
        "            generated_ids = model.generate(pixel_values, max_length=128)\n",
        "            preds = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "            refs = batch['text']\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_refs.extend(refs)\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "    # Calculate metrics\n",
        "    try:\n",
        "        val_cer = cer(all_refs, all_preds)\n",
        "        val_wer = wer(all_refs, all_preds)\n",
        "    except:\n",
        "        val_cer = 0.0\n",
        "        val_wer = 0.0\n",
        "\n",
        "    history['val_loss'].append(avg_val_loss)\n",
        "    history['val_cer'].append(val_cer)\n",
        "    history['val_wer'].append(val_wer)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nEpoch {epoch} Results:\")\n",
        "    print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
        "    print(f\"  Val Loss:   {avg_val_loss:.4f}\")\n",
        "    print(f\"  Val CER:    {val_cer:.4f} ({val_cer*100:.2f}%)\")\n",
        "    print(f\"  Val WER:    {val_wer:.4f} ({val_wer*100:.2f}%)\")\n",
        "\n",
        "    # Show sample predictions\n",
        "    print(f\"\\nSample Predictions:\")\n",
        "    for i in range(min(3, len(all_preds))):\n",
        "        print(f\"\\n  [{i+1}]\")\n",
        "        print(f\"  Reference:  {all_refs[i]}\")\n",
        "        print(f\"  Prediction: {all_preds[i]}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING COMPLETED!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-22",
      "metadata": {
        "id": "cell-22"
      },
      "source": [
        "# Plot training history\n",
        "if history['train_loss']:\n",
        "    # Create experiments directory in current working directory\n",
        "    Path('./experiments').mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    # Loss\n",
        "    axes[0, 0].plot(epochs_range, history['train_loss'], 'o-', label='Train Loss', linewidth=2, markersize=8)\n",
        "    axes[0, 0].plot(epochs_range, history['val_loss'], 's-', label='Val Loss', linewidth=2, markersize=8)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Training and Validation Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # CER\n",
        "    axes[0, 1].plot(epochs_range, [c*100 for c in history['val_cer']], 'o-',\n",
        "                    color='#e74c3c', linewidth=2, markersize=8)\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('CER (%)')\n",
        "    axes[0, 1].set_title('Character Error Rate')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # WER\n",
        "    axes[1, 0].plot(epochs_range, [w*100 for w in history['val_wer']], 'o-',\n",
        "                    color='#f39c12', linewidth=2, markersize=8)\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('WER (%)')\n",
        "    axes[1, 0].set_title('Word Error Rate')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning Rate\n",
        "    axes[1, 1].plot(range(len(history['learning_rate'])), history['learning_rate'],\n",
        "                    color='#9b59b6', linewidth=2)\n",
        "    axes[1, 1].set_xlabel('Step')\n",
        "    axes[1, 1].set_ylabel('Learning Rate')\n",
        "    axes[1, 1].set_title('Learning Rate Schedule')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('./experiments/training_metrics_correct.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"✓ Plots saved to ./experiments/training_metrics_correct.png\")\n",
        "else:\n",
        "    print(\"⚠ No training history available. Run training first.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-23",
      "metadata": {
        "id": "cell-23"
      },
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "if history['train_loss']:\n",
        "    # Create experiments directory\n",
        "    Path('../experiments').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "    # Loss\n",
        "    axes[0, 0].plot(epochs_range, history['train_loss'], 'o-', label='Train Loss', linewidth=2, markersize=8)\n",
        "    axes[0, 0].plot(epochs_range, history['val_loss'], 's-', label='Val Loss', linewidth=2, markersize=8)\n",
        "    axes[0, 0].set_xlabel('Epoch')\n",
        "    axes[0, 0].set_ylabel('Loss')\n",
        "    axes[0, 0].set_title('Training and Validation Loss')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # CER\n",
        "    axes[0, 1].plot(epochs_range, [c*100 for c in history['val_cer']], 'o-',\n",
        "                    color='#e74c3c', linewidth=2, markersize=8)\n",
        "    axes[0, 1].set_xlabel('Epoch')\n",
        "    axes[0, 1].set_ylabel('CER (%)')\n",
        "    axes[0, 1].set_title('Character Error Rate')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # WER\n",
        "    axes[1, 0].plot(epochs_range, [w*100 for w in history['val_wer']], 'o-',\n",
        "                    color='#f39c12', linewidth=2, markersize=8)\n",
        "    axes[1, 0].set_xlabel('Epoch')\n",
        "    axes[1, 0].set_ylabel('WER (%)')\n",
        "    axes[1, 0].set_title('Word Error Rate')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Learning Rate\n",
        "    axes[1, 1].plot(range(len(history['learning_rate'])), history['learning_rate'],\n",
        "                    color='#9b59b6', linewidth=2)\n",
        "    axes[1, 1].set_xlabel('Step')\n",
        "    axes[1, 1].set_ylabel('Learning Rate')\n",
        "    axes[1, 1].set_title('Learning Rate Schedule')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('../experiments/training_metrics_correct.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"✓ Plots saved to experiments/training_metrics_correct.png\")\n",
        "else:\n",
        "    print(\"⚠ No training history available. Run training first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-24",
      "metadata": {
        "id": "cell-24"
      },
      "source": [
        "# Save model\n",
        "output_dir = \"./experiments/trocr_iam_correct\"\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(output_dir)\n",
        "processor.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"✓ Model saved to {output_dir}\")\n",
        "\n",
        "# Save training history\n",
        "import json\n",
        "with open(f\"{output_dir}/training_history.json\", 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "print(f\"✓ Training history saved to {output_dir}/training_history.json\")\n",
        "\n",
        "# Optional: Mount Google Drive to save permanently\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TIP: To save to Google Drive, run:\")\n",
        "print(\"  from google.colab import drive\")\n",
        "print(\"  drive.mount('/content/drive')\")\n",
        "print(\"  Then copy the experiments folder to your Drive\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-25",
      "metadata": {
        "id": "cell-25"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "output_dir = \"../experiments/trocr_iam_correct\"\n",
        "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(output_dir)\n",
        "processor.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"✓ Model saved to {output_dir}\")\n",
        "\n",
        "# Save training history\n",
        "import json\n",
        "with open(f\"{output_dir}/training_history.json\", 'w') as f:\n",
        "    json.dump(history, f, indent=2)\n",
        "\n",
        "print(f\"✓ Training history saved to {output_dir}/training_history.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-26",
      "metadata": {
        "id": "cell-26"
      },
      "source": [
        "# Test on a sample\n",
        "model.eval()\n",
        "\n",
        "# Get a test sample\n",
        "test_dataset = IAMHandwritingDataset(\n",
        "    data_dir,  # Use the downloaded dataset path\n",
        "    split='test',\n",
        "    processor=None,\n",
        "    num_samples=3\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "\n",
        "for idx in range(3):\n",
        "    sample = test_dataset[idx]\n",
        "    handwritten_img = sample['image']\n",
        "    ground_truth = sample['text']\n",
        "    \n",
        "    # Predict\n",
        "    pixel_values = processor(handwritten_img, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(pixel_values, max_length=128, num_beams=5)\n",
        "        predicted_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    \n",
        "    # Display\n",
        "    axes[idx].imshow(handwritten_img, cmap='gray')\n",
        "    axes[idx].set_title(\n",
        "        f\"Ground Truth: {ground_truth}\\nPredicted: {predicted_text}\",\n",
        "        fontsize=11, loc='left'\n",
        "    )\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Model Predictions on Test Set', fontsize=14, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INFERENCE TEST COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-27",
      "metadata": {
        "id": "cell-27"
      },
      "outputs": [],
      "source": [
        "# Test on a sample\n",
        "model.eval()\n",
        "\n",
        "# Get a test sample\n",
        "test_dataset = IAMHandwritingDataset(\n",
        "    \"../archive/data\",\n",
        "    split='test',\n",
        "    processor=None,\n",
        "    num_samples=3\n",
        ")\n",
        "\n",
        "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "\n",
        "for idx in range(3):\n",
        "    sample = test_dataset[idx]\n",
        "    handwritten_img = sample['image']\n",
        "    ground_truth = sample['text']\n",
        "\n",
        "    # Predict\n",
        "    pixel_values = processor(handwritten_img, return_tensors=\"pt\").pixel_values.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(pixel_values, max_length=128, num_beams=5)\n",
        "        predicted_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Display\n",
        "    axes[idx].imshow(handwritten_img, cmap='gray')\n",
        "    axes[idx].set_title(\n",
        "        f\"Ground Truth: {ground_truth}\\nPredicted: {predicted_text}\",\n",
        "        fontsize=11, loc='left'\n",
        "    )\n",
        "    axes[idx].axis('off')\n",
        "\n",
        "plt.suptitle('Model Predictions on Test Set', fontsize=14, weight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INFERENCE TEST COMPLETE\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-28",
      "metadata": {
        "id": "cell-28"
      },
      "source": [
        "## 14. Summary\n",
        "\n",
        "### What We Did Correctly:\n",
        "\n",
        "1. ✅ **Understood IAM dataset structure**:\n",
        "   - Top half = Printed text (ground truth)\n",
        "   - Bottom half = Handwritten text (training input)\n",
        "\n",
        "2. ✅ **Created proper data pipeline**:\n",
        "   - Split images horizontally\n",
        "   - Extract labels from printed text using OCR\n",
        "   - Use handwritten images for training\n",
        "\n",
        "3. ✅ **Trained TrOCR correctly**:\n",
        "   - Input: Handwritten images\n",
        "   - Output: Text transcriptions\n",
        "   - Labels: Extracted from printed text\n",
        "\n",
        "4. ✅ **Fixed configuration issues**:\n",
        "   - Set decoder_start_token_id\n",
        "   - Set pad_token_id\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Train on full dataset (not just 50 samples)\n",
        "- Increase epochs (10-20 for better convergence)\n",
        "- Fine-tune hyperparameters\n",
        "- Integrate with ensemble pipeline\n",
        "- Deploy to demo interface"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}